# 强化学习

强化学习主要由智能体（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）组成。智能体执行了某个动作后，环境将会转换到一个新的状态，对于该新的状态环境会给出奖励信号（正奖励或者负奖励）。随后，智能体根据新的状态和环境反馈的奖励，按照一定的策略执行新的动作。上述过程为智能体和环境通过状态、动作、奖励进行交互的方式。

智能体通过强化学习，可以知道自己在什么状态下，应该采取什么样的动作使得自身获得最大奖励。由于智能体与环境的交互方式与人类与环境的交互方式类似，可以认为强化学习是一套通用的学习框架，可用来解决通用人工智能的问题。因此强化学习也被称为通用人工智能的机器学习方法。

![图解](https://pic1.zhimg.com/v2-f4b4fd8673a40259439cbef6b233e85c_b.jpg)





## 马尔可夫决策过程(Markov Decision Process)

我们认为强化学习整个过程是符合马尔可夫决策过程的。

MDP核心思想就是下一步的State只和当前的状态State以及当前状态将要采取的Action有关，只回溯一步。我们已知当前的State和将要采取的Action，就可以推出下一步的State是什么，而不需要继续回溯上上步的State以及Action是什么，再结合当前的（State，Action）才能得出下一步State。



## 强化学习方法

强化学习方法分为Model-free和Model-based方法

在agent执行它的动作之前，它是否能对下一步的状态和回报做出预测，如果可以，那么就是model-based方法，如果不能，即为model-free方法



## 强化学习算法(model-free)

### Value Based

基于每个State下可以采取的所有Action，这些Action对应的Value, 来选择当前State如何行动。这里的Value并不是从当前State进入下一个State时环境给的Reward，Reward是Value组成的一部分。但我们实际训练时既要关注当前的收益，也要关注长远的收益，所以这里面的Value是通过一个计算公式得出来的，而不仅仅是状态变更环境立即反馈的Reward。计算value比较复杂，通常使用贝尔曼方程

Value值在训练时是未知的，一般设置为0，通过让agent不断的去尝试各种Action不断获得Reward，然后通过计算value的公式不断地更新Value。最终在训练N多轮以后，Value值会趋于一个稳定的数字，才能得出具体的State下，采取特定Action，对应的Value是多少

算法：

- Q-Learning 

- DQN

- Double DQN （论文：[Deep reinforcement learning with double q-learning](https://link.zhihu.com/?target=https%3A//ojs.aaai.org/index.php/AAAI/article/view/10295)）

- Dueling DQN（论文：[Dueling network architectures for deep reinforcement learning](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1511.06581.pdf)）

- QR-DQN（论文：[Distributional Reinforcement Learning with Quantile Regression](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1710.10044.pdf)）

- Rainbow（论文：[Rainbow: Combining Improvements in Deep Reinforcement Learning](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1710.02298.pdf)）

  具体可看[图解DQN，DDQN，DDPG网络](https://zhuanlan.zhihu.com/p/362076700)

 



### Policy Based

基于每个State可以采取的Action策略，针对Action策略进行建模，学习出具体State下可以采取的Action对应的概率，然后根据概率来选择Action。

具体可参考：[基于值和策略的强化学习入坑 ](https://zhuanlan.zhihu.com/p/54825295)



### Actor-Critic

AC分类就是将Value-Based和Policy-Based结合在一起，里面的算法结合了两者。

算法：

- A2C和A3C（A3C论文：[Asynchronous Methods for Deep Reinforcement Learning](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1602.01783.pdf)）
- TRPO（论文：[Trust Region Policy Optimization](https://link.zhihu.com/?target=http%3A//proceedings.mlr.press/v37/schulman15.pdf)）
- PPO（论文：[Proximal Policy Optimization Algorithms](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1707.06347.pdf)）
- SAC（论文：[Soft Actor-Critic Algorithms and Applications](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1812.05905.pdf)，[Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1801.01290.pdf)）
- DPG（论文：[Deterministic Policy Gradient Algorithms](https://link.zhihu.com/?target=http%3A//proceedings.mlr.press/v32/silver14.pdf)）
- DDPG（论文：[Continuous control with deep reinforcement learning](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1509.02971.pdf%3Fsource%3Dpost_page---------------------------)）
- TD3（论文：[Addressing Function Approximation Error in Actor-Critic Methods](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1802.09477.pdf)）









## Experience replay(经验回放)

将系统探索环境得到的数据储存起来，然后随机采样样本更新深度神经网络的参数。

在强化学习里面，针对reward的设计，通常有两种(1)只有完成某一个任务才给予奖励，否则没有奖励。（2）人为的定义一个奖励函数，比如让机器人到达一个目标，距离目标越近，奖励越高。但是（1）会导致智能体能得到的奖励的时候很少，很稀疏，从人类的角度来看，就是机器会特别“沮丧”，因为它大部分尝试都是无用功。（2）这种函数的设置则极其依赖人类的聪明才智，不够通用。

我们再来想人类这种生物是如何处理之前做过的事情的，而且会反思，并且每一次反思得出的结论可能是不一样的，这正所谓经验回放。把这个想法运用到DQN中，每一次做出动作，进入下一个状态，我不会马上用这个来训练我们的DQN，而是把其放入“经历池”中，每次从经历池中取出一部分数据，用作训练。这个样子我们就可以充分地利用数据，提高训练效率，另外，因为我们随机抽取数据，数据是基本上没有相关性的，因此训练更容易收敛。


Experience Replay主要作用是克服经验数据的相关性（correlated data）和非平稳分布（non-stationary distribution）问题。它的做法是从以往的状态转移（经验）中随机采样进行训练。

优点：

1. 数据利用率高，因为一个样本被多次使用。
2. 连续样本的相关性会使参数更新的方差（variance）比较大，该机制可减少这种相关性。注意这里用的是随机采样，这也给之后的改进埋下了伏笔。





## exploration-exploitation dilemma(探索利用困境)

- Exploitation：利用当前已知信息做决策
- Exploration：探索未知空间获取更多信息





 Playing Atari with Deep Reinforcement Learning

这是首篇结合神经网络以及强化学习的文章，采用了CNN网络结构

最早DQN使用了Experience Replay的方法。

一个卷积神经网络，基于Q-learning进行训练，它的输入就是像素点，输出是一个可以预估未来奖励的值函数。

背景：强化学习Q-learning很好使，但是它不适宜处理高维数据，奖励函数需要人为的去设定，可以用神经网络去拟合一个奖励函数

玩一个游戏，一个关卡和下一个关卡的状态分布是不同的，所以训练好了前一个关卡，下一个关卡又要重新训练















## ReinForce算法

Reinfoce算法是基于MC更新方式的Policy Gradient算法。每完成一个episode才进行算法的更新，它是基于策略梯度的一种算法，策略梯度算法是指先找到一个评价指标，然后利用随机梯度上升的方法来更新参数使评价指标不断的上升。

- 关键思想：学习过程中，产生良好结果的动作的可能性逐渐增加，而产生不良结果的动作的可能性逐渐降低。，动作概率通过策略梯度来迭代转换。
